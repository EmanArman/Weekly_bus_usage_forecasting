{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-30T18:09:05.968965Z","iopub.execute_input":"2023-04-30T18:09:05.969762Z","iopub.status.idle":"2023-04-30T18:09:05.981500Z","shell.execute_reply.started":"2023-04-30T18:09:05.969718Z","shell.execute_reply":"2023-04-30T18:09:05.979917Z"},"trusted":true},"execution_count":323,"outputs":[{"name":"stdout","text":"/kaggle/input/piworksbus/municipality_bus_utilization.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_percentage_error","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:09:05.993050Z","iopub.execute_input":"2023-04-30T18:09:05.995733Z","iopub.status.idle":"2023-04-30T18:09:06.002606Z","shell.execute_reply.started":"2023-04-30T18:09:05.995685Z","shell.execute_reply":"2023-04-30T18:09:06.001342Z"},"trusted":true},"execution_count":324,"outputs":[]},{"cell_type":"code","source":"#upload the dataset\nurl= '/kaggle/input/piworksbus/municipality_bus_utilization.csv'\ndf= pd.read_csv(url,sep=',')\n\n#Deal with missing values using interpolation\ndf.interpolate(method='linear', axis=0, limit=None, inplace=True)\n\n#Put in date-time format\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n#set the date as index\ndf.set_index('timestamp',drop=False,inplace=True)\n\n#resample the data hourly regarding to the max value\nhourly_data = df.resample('H').max()\n\n# Add lag features, as baseline simple solution we cad only one lag factor which is usage_LAG_-1 to be the target\nfor i in range(-1, 5):\n    hourly_data[f'usage_LAG_{i}'] = hourly_data['usage'].shift(i)\n\nhourly_data.dropna(inplace=True)\nhourly_data","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:09:06.017120Z","iopub.execute_input":"2023-04-30T18:09:06.017605Z","iopub.status.idle":"2023-04-30T18:09:06.087321Z","shell.execute_reply.started":"2023-04-30T18:09:06.017557Z","shell.execute_reply":"2023-04-30T18:09:06.085976Z"},"trusted":true},"execution_count":325,"outputs":[{"execution_count":325,"output_type":"execute_result","data":{"text/plain":"                              timestamp  municipality_id   usage  \\\ntimestamp                                                          \n2017-06-04 11:00:00 2017-06-04 11:59:44              9.0  3257.0   \n2017-06-04 12:00:00 2017-06-04 12:29:45              9.0  3260.0   \n2017-06-04 13:00:00 2017-06-04 13:29:45              9.0  3241.0   \n2017-06-04 14:00:00 2017-06-04 14:57:13              9.0  3154.0   \n2017-06-04 15:00:00 2017-06-04 15:30:14              9.0  2848.0   \n...                                 ...              ...     ...   \n2017-08-18 15:00:00 2017-08-18 15:30:24              9.0  1764.0   \n2017-08-19 12:00:00 2017-08-19 12:30:32              9.0  3157.0   \n2017-08-19 13:00:00 2017-08-19 13:30:35              9.0  3194.0   \n2017-08-19 14:00:00 2017-08-19 14:30:33              9.0  3183.0   \n2017-08-19 15:00:00 2017-08-19 15:29:33              9.0  3111.0   \n\n                     total_capacity  usage_LAG_-1  usage_LAG_0  usage_LAG_1  \\\ntimestamp                                                                     \n2017-06-04 11:00:00          3893.0        3260.0       3257.0       3178.0   \n2017-06-04 12:00:00          3893.0        3241.0       3260.0       3257.0   \n2017-06-04 13:00:00          3893.0        3154.0       3241.0       3260.0   \n2017-06-04 14:00:00          3893.0        2848.0       3154.0       3241.0   \n2017-06-04 15:00:00          3893.0        2665.0       2848.0       3154.0   \n...                             ...           ...          ...          ...   \n2017-08-18 15:00:00          3893.0        1647.0       1764.0       1893.0   \n2017-08-19 12:00:00          3893.0        3194.0       3157.0       2986.0   \n2017-08-19 13:00:00          3893.0        3183.0       3194.0       3157.0   \n2017-08-19 14:00:00          3893.0        3111.0       3183.0       3194.0   \n2017-08-19 15:00:00          3893.0        2779.0       3111.0       3183.0   \n\n                     usage_LAG_2  usage_LAG_3  usage_LAG_4  \ntimestamp                                                   \n2017-06-04 11:00:00       2811.0       2016.0       1090.0  \n2017-06-04 12:00:00       3178.0       2811.0       2016.0  \n2017-06-04 13:00:00       3257.0       3178.0       2811.0  \n2017-06-04 14:00:00       3260.0       3257.0       3178.0  \n2017-06-04 15:00:00       3241.0       3260.0       3257.0  \n...                          ...          ...          ...  \n2017-08-18 15:00:00       1819.0       1723.0       1387.0  \n2017-08-19 12:00:00       2785.0       2613.0       1724.0  \n2017-08-19 13:00:00       2986.0       2785.0       2613.0  \n2017-08-19 14:00:00       3157.0       2986.0       2785.0  \n2017-08-19 15:00:00       3194.0       3157.0       2986.0  \n\n[314 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>municipality_id</th>\n      <th>usage</th>\n      <th>total_capacity</th>\n      <th>usage_LAG_-1</th>\n      <th>usage_LAG_0</th>\n      <th>usage_LAG_1</th>\n      <th>usage_LAG_2</th>\n      <th>usage_LAG_3</th>\n      <th>usage_LAG_4</th>\n    </tr>\n    <tr>\n      <th>timestamp</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-06-04 11:00:00</th>\n      <td>2017-06-04 11:59:44</td>\n      <td>9.0</td>\n      <td>3257.0</td>\n      <td>3893.0</td>\n      <td>3260.0</td>\n      <td>3257.0</td>\n      <td>3178.0</td>\n      <td>2811.0</td>\n      <td>2016.0</td>\n      <td>1090.0</td>\n    </tr>\n    <tr>\n      <th>2017-06-04 12:00:00</th>\n      <td>2017-06-04 12:29:45</td>\n      <td>9.0</td>\n      <td>3260.0</td>\n      <td>3893.0</td>\n      <td>3241.0</td>\n      <td>3260.0</td>\n      <td>3257.0</td>\n      <td>3178.0</td>\n      <td>2811.0</td>\n      <td>2016.0</td>\n    </tr>\n    <tr>\n      <th>2017-06-04 13:00:00</th>\n      <td>2017-06-04 13:29:45</td>\n      <td>9.0</td>\n      <td>3241.0</td>\n      <td>3893.0</td>\n      <td>3154.0</td>\n      <td>3241.0</td>\n      <td>3260.0</td>\n      <td>3257.0</td>\n      <td>3178.0</td>\n      <td>2811.0</td>\n    </tr>\n    <tr>\n      <th>2017-06-04 14:00:00</th>\n      <td>2017-06-04 14:57:13</td>\n      <td>9.0</td>\n      <td>3154.0</td>\n      <td>3893.0</td>\n      <td>2848.0</td>\n      <td>3154.0</td>\n      <td>3241.0</td>\n      <td>3260.0</td>\n      <td>3257.0</td>\n      <td>3178.0</td>\n    </tr>\n    <tr>\n      <th>2017-06-04 15:00:00</th>\n      <td>2017-06-04 15:30:14</td>\n      <td>9.0</td>\n      <td>2848.0</td>\n      <td>3893.0</td>\n      <td>2665.0</td>\n      <td>2848.0</td>\n      <td>3154.0</td>\n      <td>3241.0</td>\n      <td>3260.0</td>\n      <td>3257.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2017-08-18 15:00:00</th>\n      <td>2017-08-18 15:30:24</td>\n      <td>9.0</td>\n      <td>1764.0</td>\n      <td>3893.0</td>\n      <td>1647.0</td>\n      <td>1764.0</td>\n      <td>1893.0</td>\n      <td>1819.0</td>\n      <td>1723.0</td>\n      <td>1387.0</td>\n    </tr>\n    <tr>\n      <th>2017-08-19 12:00:00</th>\n      <td>2017-08-19 12:30:32</td>\n      <td>9.0</td>\n      <td>3157.0</td>\n      <td>3893.0</td>\n      <td>3194.0</td>\n      <td>3157.0</td>\n      <td>2986.0</td>\n      <td>2785.0</td>\n      <td>2613.0</td>\n      <td>1724.0</td>\n    </tr>\n    <tr>\n      <th>2017-08-19 13:00:00</th>\n      <td>2017-08-19 13:30:35</td>\n      <td>9.0</td>\n      <td>3194.0</td>\n      <td>3893.0</td>\n      <td>3183.0</td>\n      <td>3194.0</td>\n      <td>3157.0</td>\n      <td>2986.0</td>\n      <td>2785.0</td>\n      <td>2613.0</td>\n    </tr>\n    <tr>\n      <th>2017-08-19 14:00:00</th>\n      <td>2017-08-19 14:30:33</td>\n      <td>9.0</td>\n      <td>3183.0</td>\n      <td>3893.0</td>\n      <td>3111.0</td>\n      <td>3183.0</td>\n      <td>3194.0</td>\n      <td>3157.0</td>\n      <td>2986.0</td>\n      <td>2785.0</td>\n    </tr>\n    <tr>\n      <th>2017-08-19 15:00:00</th>\n      <td>2017-08-19 15:29:33</td>\n      <td>9.0</td>\n      <td>3111.0</td>\n      <td>3893.0</td>\n      <td>2779.0</td>\n      <td>3111.0</td>\n      <td>3183.0</td>\n      <td>3194.0</td>\n      <td>3157.0</td>\n      <td>2986.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>314 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Split data into training and testing sets\ntrain_start_date = '2017-06-04 00:00:00'\ntrain_end_date = '2017-08-04 23:00:00'\ntest_start_date = '2017-08-05 00:00:00'\ntest_end_date = '2017-08-19 23:00:00'\nlast_week_start_date= '2017-08-13 00:00:00'\n\ntrain_data = hourly_data.loc[(hourly_data['timestamp'] >= train_start_date) & (hourly_data['timestamp'] <= train_end_date)]\ntest_data = hourly_data.loc[(hourly_data['timestamp'] >= test_start_date) & (hourly_data['timestamp'] <= test_end_date)]\n#prepare data of only last week for the main task\nlast_week_data= hourly_data.loc[(hourly_data['timestamp'] >= last_week_start_date) & (hourly_data['timestamp'] <= test_end_date)]\n\n\n# Define features and target\nX_train = train_data.drop(['usage_LAG_0','usage_LAG_-1','timestamp'], axis=1).values \ny_train = (train_data['usage_LAG_-1']).values\nX_test = test_data.drop([ 'usage_LAG_0','usage_LAG_-1','timestamp'], axis=1).values\ny_test = (test_data['usage_LAG_-1']).values\nX_last_week = last_week_data.drop([ 'usage_LAG_0','usage_LAG_-1','timestamp'], axis=1).values\n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:09:06.089578Z","iopub.execute_input":"2023-04-30T18:09:06.090815Z","iopub.status.idle":"2023-04-30T18:09:06.111622Z","shell.execute_reply.started":"2023-04-30T18:09:06.090748Z","shell.execute_reply":"2023-04-30T18:09:06.109899Z"},"trusted":true},"execution_count":326,"outputs":[]},{"cell_type":"code","source":"#common function to evaluate the models\ndef evaluate(y_test,y_pred):\n    #calculate the R^2 value\n    r2 = r2_score(y_test, y_pred)*100\n    #calculate the MAPE value\n    mape= mean_absolute_percentage_error(y_test,y_pred)*100\n    \n    #print the results\n    print(f\"The R\\u00b2 value is: {r2:0.4f}%\")\n    print(f\"The MAPE value is: {mape:0.4f}%\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:09:06.113570Z","iopub.execute_input":"2023-04-30T18:09:06.114071Z","iopub.status.idle":"2023-04-30T18:09:06.125920Z","shell.execute_reply.started":"2023-04-30T18:09:06.114017Z","shell.execute_reply":"2023-04-30T18:09:06.124441Z"},"trusted":true},"execution_count":327,"outputs":[]},{"cell_type":"code","source":"# Fit and evaluate linear regression model as simple baseline method\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n#predict and evaluate the test data\ny_pred_lr = lr.predict(X_test)\n\nprint('Linear regression model evaluation')\nevaluate(y_test,y_pred_lr)\n\n#Forecast hourly data for the next week\ny_forecasted_week_lr = lr.predict(X_last_week)\nprint(f\"Linear regression model forecasted week usage:{np.round(y_forecasted_week_lr, 1)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:09:06.128738Z","iopub.execute_input":"2023-04-30T18:09:06.130091Z","iopub.status.idle":"2023-04-30T18:09:06.143491Z","shell.execute_reply.started":"2023-04-30T18:09:06.130032Z","shell.execute_reply":"2023-04-30T18:09:06.142042Z"},"trusted":true},"execution_count":328,"outputs":[{"name":"stdout","text":"Linear regression model evaluation\nThe R² value is: 96.7478%\nThe MAPE value is: 3.4116%\nLinear regression model forecasted week usage:[3602.7 3320.  3119.1 2745.8 3302.  3285.  3124.5 2620.4 3320.4 3401.7\n 3271.6 3184.7 2907.7 2999.5 2973.3 2839.8 2543.3 2170.7 1693.6 1873.8\n 1976.9 2171.1 2150.5 1887.1 1857.6 1836.6 1625.4 3293.4 3083.1 3014.\n 2893.1]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fit and evaluate linear regression model as simple baseline method\ngb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\ngb.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred_gb = gb.predict(X_test)\n\nprint('Gradient boosting model evaluation')\nevaluate(y_test,y_pred_gb)\n\n#Forecast hourly data for the next week\ny_forecasted_week_gb = gb.predict(X_last_week)\nprint(f\"Gradient boosting model forecasted week usage:{np.round(y_forecasted_week_gb, 1)}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-30T18:09:06.145481Z","iopub.execute_input":"2023-04-30T18:09:06.146280Z","iopub.status.idle":"2023-04-30T18:09:06.256337Z","shell.execute_reply.started":"2023-04-30T18:09:06.146228Z","shell.execute_reply":"2023-04-30T18:09:06.254928Z"},"trusted":true},"execution_count":329,"outputs":[{"name":"stdout","text":"Gradient boosting model evaluation\nThe R² value is: 97.0967%\nThe MAPE value is: 2.8612%\nGradient boosting model forecasted week usage:[3475.2 3429.6 3136.6 2725.8 3369.4 3320.8 3132.1 2550.7 3330.  3364.8\n 3303.  3248.  2870.3 2959.4 2972.1 2782.1 2604.3 2150.5 1660.2 1751.4\n 1751.6 2118.6 2050.2 1819.6 1855.2 1720.4 1646.8 3122.9 3141.3 3113.3\n 3006. ]\n","output_type":"stream"}]}]}